{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[PyCUDA](https://mathema.tician.de/software/pycuda/) is a Python wrapper around CUDA, NVidia's extension of C/C++ for GPUs.\n",
    "\n",
    "There's also a [PyOpenCL](https://mathema.tician.de/software/pyopencl/) for the vendor-independent OpenCL standard.\n",
    "\n",
    "\n",
    "Following shows the CUDA parallel thread hierarchy. CUDA executes kernels using a grid of blocks of threads. This figure shows the common indexing pattern used in CUDA programs using the CUDA keywords `gridDim.x` (the number of thread blocks), `blockDim.x` (the number of threads in each block), `blockIdx.x` (the index the current block within the grid), and `threadIdx.`x (the index of the current thread within the block).\n",
    "\n",
    "<img src=\"assets/cuda_indexing.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you can use PyCuda, you have to import and initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy\n",
    "\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you do not have to use `pycuda.autoinit` - initialization, context creation, and cleanup can also be performed manually, if desired\n",
    "\n",
    "For this tutorial, we'll stick to something simple: we will write code to double each entry in array. To this end, we write the corresponding CUDA C code, and feed it into the constructor of a `pycuda.compiler.SourceModule`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate a CUDA (C-ish) function that will run on the GPU; PROBLEM_SIZE is hard-wired\n",
    "module = SourceModule(\"\"\"\n",
    "  __global__ void doublify(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] *= 2;\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "# pull \"doublify\" out as a Python callable\n",
    "doublify = module.get_function(\"doublify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will create some data as numpy arrays on the CPU.\n",
    "\n",
    "The next step in this and most other programs is to transfer data onto the device. In PyCuda, you will mostly transfer data from numpy arrays on the host. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create Numpy arrays on the CPU\n",
    "a = numpy.random.randn(4,4).astype(numpy.float32)\n",
    "\n",
    "#next, we need somewhere to transfer data to, so we need to allocate memory on the device:\n",
    "a_gpu = cuda.mem_alloc(a.nbytes)\n",
    "\n",
    "#as a last step, we need to transfer the data to the GPU:\n",
    "\n",
    "cuda.memcpy_htod(a_gpu, a)\n",
    "\n",
    "\n",
    "# define block/grid size for our problem: 4x4 block size\n",
    "blockdim = (4,4,1)\n",
    "#griddim = (int(math.ceil(PROBLEM_SIZE / 512.0)), 1, 1)\n",
    "\n",
    "# copy the \"driver.In\" arrays to the GPU, run the \n",
    "#just_multiply(driver.Out(dest), driver.In(a), driver.In(b), block=blockdim, grid=griddim)\n",
    "doublify(a_gpu, block=blockdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.23081899 -1.60914695 -0.21349314 -0.58991599]\n",
      " [-1.53882289  3.44664907  1.44528294 -3.1512928 ]\n",
      " [-2.77189636  0.24643208 -1.41279054  0.12612192]\n",
      " [ 0.43340847 -3.97955871 -2.67981982  0.26493114]]\n",
      "[[-0.61540949 -0.80457348 -0.10674657 -0.294958  ]\n",
      " [-0.76941144  1.72332454  0.72264147 -1.5756464 ]\n",
      " [-1.38594818  0.12321604 -0.70639527  0.06306096]\n",
      " [ 0.21670423 -1.98977935 -1.33990991  0.13246557]]\n"
     ]
    }
   ],
   "source": [
    "#Finally, we fetch the data back from the GPU and display it, together with the original a:\n",
    "\n",
    "a_doubled = numpy.empty_like(a)\n",
    "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "print (a_doubled)\n",
    "print (a)\n",
    "#doublify(cuda.InOut(a), block=blockdim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's do that calculation of $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141594"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROBLEM_SIZE = int(1e6)\n",
    "\n",
    "module2 = SourceModule(\"\"\"\n",
    "__global__ void mapper(float *dest)\n",
    "{\n",
    "  const int id = threadIdx.x + blockDim.x*blockIdx.x;\n",
    "  const double x = 1.0 * id / %d;     // x goes from 0.0 to 1.0 in PROBLEM_SIZE steps\n",
    "  if (id < %d)\n",
    "    dest[id] = 4.0 / (1.0 + x*x);\n",
    "}\n",
    "\"\"\" % (PROBLEM_SIZE, PROBLEM_SIZE))\n",
    "\n",
    "mapper = module2.get_function(\"mapper\")\n",
    "dest = numpy.empty(PROBLEM_SIZE, dtype=numpy.float32)\n",
    "blockdim = (512, 1, 1)\n",
    "griddim = (int(math.ceil(PROBLEM_SIZE / 512.0)), 1, 1)\n",
    "\n",
    "mapper(cuda.Out(dest), block=blockdim, grid=griddim)\n",
    "\n",
    "dest.sum() * (1.0 / PROBLEM_SIZE)  # correct for bin size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're doing the mapper (problem of size 1 million) on the GPU and the final sum (problem of size 1 million) on the CPU.\n",
    "\n",
    "However, we want to do all the big data work on the GPU.\n",
    "\n",
    "On the next slide is an algorithm that merges array elements with their neighbors in $\\log_2(\\mbox{million}) = 20$ steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1415934999999999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module3 = SourceModule(\"\"\"\n",
    "__global__ void reducer(float *dest, int i)\n",
    "{\n",
    "  const int PROBLEM_SIZE = %d;\n",
    "  const int id = threadIdx.x + blockDim.x*blockIdx.x;\n",
    "  if (id %% (2*i) == 0  &&  id + i < PROBLEM_SIZE) {\n",
    "    dest[id] += dest[id + i];\n",
    "  }\n",
    "}\n",
    "\"\"\" % PROBLEM_SIZE)\n",
    "\n",
    "blockdim = (512, 1, 1)\n",
    "griddim = (int(math.ceil(PROBLEM_SIZE / 512.0)), 1, 1)\n",
    "\n",
    "reducer = module3.get_function(\"reducer\")\n",
    "\n",
    "# Python for loop over the 20 steps to reduce the array\n",
    "i = 1\n",
    "while i < PROBLEM_SIZE:\n",
    "    reducer(cuda.InOut(dest), numpy.int32(i), block=blockdim, grid=griddim)\n",
    "    i *= 2\n",
    "\n",
    "# final result is in the first element\n",
    "dest[0] * (1.0 / PROBLEM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The only problem now is that we're copying this `dest` array back and forth between the CPU and GPU. Let's fix that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415935\n"
     ]
    }
   ],
   "source": [
    "# allocate the array directly on the GPU, no CPU involved\n",
    "dest_gpu = cuda.mem_alloc(PROBLEM_SIZE * numpy.dtype(numpy.float32).itemsize)\n",
    "\n",
    "# do it again without \"driver.InOut\", which copies Numpy (CPU) to and from the GPU\n",
    "mapper(dest_gpu, block=blockdim, grid=griddim)\n",
    "i = 1\n",
    "while i < PROBLEM_SIZE:\n",
    "    reducer(dest_gpu, numpy.int32(i), block=blockdim, grid=griddim)\n",
    "    i *= 2\n",
    "\n",
    "# we only need the first element, so create a Numpy array with exactly one element\n",
    "only_one_element = numpy.empty(1, dtype=numpy.float32)\n",
    "\n",
    "# copy just that one element\n",
    "cuda.memcpy_dtoh(only_one_element, dest_gpu)\n",
    "\n",
    "print (only_one_element[0] * (1.0 / PROBLEM_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
